{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_tweet, lookup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_count_tweets(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\n",
    "                \"result\": {},\n",
    "                \"tweets\": [\n",
    "                    \"i am happy\",\n",
    "                    \"i am tricked\",\n",
    "                    \"i am sad\",\n",
    "                    \"i am tired\",\n",
    "                    \"i am tired\",\n",
    "                ],\n",
    "                \"ys\": [1, 0, 0, 0, 0],\n",
    "            },\n",
    "            \"expected\": {\n",
    "                (\"happi\", 1): 1,\n",
    "                (\"trick\", 0): 1,\n",
    "                (\"sad\", 0): 1,\n",
    "                (\"tire\", 0): 2,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"larger_check\",\n",
    "            \"input\": {\n",
    "                \"result\": {},\n",
    "                \"tweets\": [\n",
    "                    \"i am happy\",\n",
    "                    \"i am tricked\",\n",
    "                    \"i am sad\",\n",
    "                    \"i am tired\",\n",
    "                    \"i am tired but proud today\",\n",
    "                    \"i am you are\",\n",
    "                    \"you are happy\",\n",
    "                    \"he was sad\",\n",
    "                ],\n",
    "                \"ys\": [1, 0, 0, 0, 1, 0, 1, 0],\n",
    "            },\n",
    "            \"expected\": {\n",
    "                (\"happi\", 1): 2,\n",
    "                (\"trick\", 0): 1,\n",
    "                (\"sad\", 0): 2,\n",
    "                (\"tire\", 0): 1,\n",
    "                (\"tire\", 1): 1,\n",
    "                (\"proud\", 1): 1,\n",
    "                (\"today\", 1): 1,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"nonempyt_dict_check\",\n",
    "            \"input\": {\n",
    "                \"result\": {\n",
    "                    (\"happi\", 1): 3,\n",
    "                    (\"sad\", 0): 1,\n",
    "                    (\"tire\", 0): 2,\n",
    "                    (\"tire\", 1): 1,\n",
    "                },\n",
    "                \"tweets\": [\n",
    "                    \"i am happy\",\n",
    "                    \"i am tricked\",\n",
    "                    \"i am sad\",\n",
    "                    \"i am tired\",\n",
    "                    \"i am tired but proud today\",\n",
    "                    \"i am you are\",\n",
    "                    \"you are happy\",\n",
    "                    \"he was sad\",\n",
    "                ],\n",
    "                \"ys\": [1, 0, 0, 0, 1, 0, 1, 0],\n",
    "            },\n",
    "            \"expected\": {\n",
    "                (\"happi\", 1): 5,\n",
    "                (\"sad\", 0): 3,\n",
    "                (\"tire\", 0): 3,\n",
    "                (\"tire\", 1): 2,\n",
    "                (\"trick\", 0): 1,\n",
    "                (\"proud\", 1): 1,\n",
    "                (\"today\", 1): 1,\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result, dict)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\"name\": test_case[\"name\"], \"expected\": dict, \"got\": type(result),}\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type for count_tweets function. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert result == test_case[\"expected\"]\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output values in count_tweets function. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "            if test_case[\"name\"] == \"nonempyt_dict_check\":\n",
    "                print(\n",
    "                    f\"Check the use of the result dictionary.\\nRemember that it is passed as parameter and should not be initialized inside the function.\"\n",
    "                )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_naive_bayes_bk(target, freqs, train_x, train_y):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"train_x\": train_x, \"train_y\": train_y},\n",
    "            \"expected\": {\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test0.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"smaller_check\",\n",
    "            \"input\": {\n",
    "                \"freqs\": freqs,\n",
    "                \"train_x\": train_x[:10] + train_x[-10:],\n",
    "                \"train_y\": np.concatenate((train_y[:10], train_y[-10:]), axis=0),\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test1.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"smaller_unbalanced_check\",\n",
    "            \"input\": {\n",
    "                \"freqs\": freqs,\n",
    "                \"train_x\": train_x[:10] + train_x[-5:],\n",
    "                \"train_y\": np.concatenate((train_y[:10], train_y[-5:]), axis=0),\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test2.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result1, result2 = target(**test_case[\"input\"])\n",
    "\n",
    "        print(\"result2\", result2[\"encor\"])\n",
    "        print(\"expected\", test_case[\"expected\"][\"loglikelihood\"][\"encor\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result1, np.float64) or isinstance(result1, float)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": np.float64,\n",
    "                    \"got\": type(result1),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type for logprior. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(result1, test_case[\"expected\"][\"logprior\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"][\"logprior\"],\n",
    "                    \"got\": result1,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong value for logprior. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result2, dict)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\"name\": test_case[\"name\"], \"expected\": dict, \"got\": type(result2),}\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type for loglikelihood. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert len(result2) == len(test_case[\"expected\"][\"loglikelihood\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": len(test_case[\"expected\"][\"loglikelihood\"]),\n",
    "                    \"got\": len(result2),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong number of keys in loglikelihood dictionary. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(\n",
    "                result2, test_case[\"expected\"][\"loglikelihood\"], atol=1e-3\n",
    "            )\n",
    "\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": len(test_case[\"expected\"][\"loglikelihood\"]),\n",
    "                    \"got\": len(result2),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_naive_bayes(target, freqs, train_x, train_y):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"train_x\": train_x, \"train_y\": train_y},\n",
    "            \"expected\": {\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"smaller_check\",\n",
    "            \"input\": {\n",
    "                \"freqs\": freqs,\n",
    "                \"train_x\": train_x[:10] + train_x[-10:],\n",
    "                \"train_y\": np.concatenate((train_y[:10], train_y[-10:]), axis=0),\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"smaller_unbalanced_check\",\n",
    "            \"input\": {\n",
    "                \"freqs\": freqs,\n",
    "                \"train_x\": train_x[:10] + train_x[-5:],\n",
    "                \"train_y\": np.concatenate((train_y[:10], train_y[-5:]), axis=0),\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result1, result2 = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result1, np.float64) or isinstance(result1, float)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": np.float64,\n",
    "                    \"got\": type(result1),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type for logprior. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(result1, test_case[\"expected\"][\"logprior\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"][\"logprior\"],\n",
    "                    \"got\": result1,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong value for logprior. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result2, dict)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\"name\": test_case[\"name\"], \"expected\": dict, \"got\": type(result2),}\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type for loglikelihood. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert len(result2) == len(test_case[\"expected\"][\"loglikelihood\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": len(test_case[\"expected\"][\"loglikelihood\"]),\n",
    "                    \"got\": len(result2),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong number of keys in loglikelihood dictionary. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        count_good = 0\n",
    "        for key, value in test_case[\"expected\"][\"loglikelihood\"].items():\n",
    "\n",
    "            if np.isclose(result2[key], value):\n",
    "                count_good += 1\n",
    "\n",
    "        try:\n",
    "            assert count_good == len(test_case[\"expected\"][\"loglikelihood\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": len(test_case[\"expected\"][\"loglikelihood\"]),\n",
    "                    \"got\": len(result2),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes_predict(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"She smiled.\",\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 1.5577981920239676,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"neutral_example_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"She did not answer my question.\",\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 0.6025188572932385,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"negative_example_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"She did not answer my question :(\",\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": -6.924732171512272,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"positive_prior_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"She smiled.\",\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 2.2509453725839133,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"neutral_example_prior_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"She did not answer my question.\",\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 1.2956660378531841,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"negative_example_prior_check\",\n",
    "            \"input\": {\n",
    "                \"tweet\": \"She did not answer my question :(\",\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": -6.231584990952325,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert np.isclose(result, test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output value. \\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unittest_test_naive_bayes(target, test_x, test_y):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\n",
    "                \"test_x\": test_x,\n",
    "                \"test_y\": test_y,\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 0.9955,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"smaller_check\",\n",
    "            \"input\": {\n",
    "                \"test_x\": test_x[:100] + test_x[-100:],\n",
    "                \"test_y\": np.concatenate((test_y[:100], test_y[-100:]), axis=0),\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 0.995,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"smaller_prior_check\",\n",
    "            \"input\": {\n",
    "                \"test_x\": test_x[:100] + test_x[-100:],\n",
    "                \"test_y\": np.concatenate((test_y[:100], test_y[-100:]), axis=0),\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 0.995,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"small_check\",\n",
    "            \"input\": {\n",
    "                \"test_x\": test_x[:150] + test_x[-100:],\n",
    "                \"test_y\": np.concatenate((test_y[:150], test_y[-100:]), axis=0),\n",
    "                \"logprior\": 0.0,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 0.996,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"small_prior_check\",\n",
    "            \"input\": {\n",
    "                \"test_x\": test_x[:150] + test_x[-100:],\n",
    "                \"test_y\": np.concatenate((test_y[:150], test_y[-100:]), axis=0),\n",
    "                \"logprior\": 0.6931471805599456,\n",
    "                \"loglikelihood\": pickle.load(\n",
    "                    open(\"./support_files/loglikelihood_test.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 0.996,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "        try:\n",
    "            assert np.isclose(result, test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output value for accuracy.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_ratio(target, freqs):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"word\": \"happi\"},\n",
    "            \"expected\": {\"positive\": 162, \"negative\": 18, \"ratio\": 8.578947368421053},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"word_bad_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"word\": \"bad\"},\n",
    "            \"expected\": {\"positive\": 14, \"negative\": 54, \"ratio\": 0.2727272727272727},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"word_fun_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"word\": \"fun\"},\n",
    "            \"expected\": {\"positive\": 45, \"negative\": 20, \"ratio\": 2.1904761904761907},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"word_sad_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"word\": \"sad\"},\n",
    "            \"expected\": {\"positive\": 5, \"negative\": 100, \"ratio\": 0.0594059405940594},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result, dict)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": type(test_case[\"expected\"]),\n",
    "                    \"got\": type(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        for key, value in test_case[\"expected\"].items():\n",
    "            try:\n",
    "                assert np.isclose(result[key], value)\n",
    "                successful_cases += 1\n",
    "            except:\n",
    "                failed_cases.append(\n",
    "                    {\n",
    "                        \"name\": test_case[\"name\"],\n",
    "                        \"expected\": {key: value},\n",
    "                        \"got\": {key: result[key]},\n",
    "                    }\n",
    "                )\n",
    "                print(\n",
    "                    f\"Wrong output values.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "                )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_words_by_threshold(target, freqs):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"default_check1\",\n",
    "            \"input\": {\"freqs\": freqs, \"label\": 0, \"threshold\": 0.05},\n",
    "            \"expected\": {\n",
    "                \":(\": {\"positive\": 1, \"negative\": 3675, \"ratio\": 0.000544069640914037},\n",
    "                \":-(\": {\"positive\": 0, \"negative\": 386, \"ratio\": 0.002583979328165375},\n",
    "                \"zayniscomingbackonjuli\": {\n",
    "                    \"positive\": 0,\n",
    "                    \"negative\": 19,\n",
    "                    \"ratio\": 0.05,\n",
    "                },\n",
    "                \"26\": {\"positive\": 0, \"negative\": 20, \"ratio\": 0.047619047619047616},\n",
    "                \">:(\": {\"positive\": 0, \"negative\": 43, \"ratio\": 0.022727272727272728},\n",
    "                \"lost\": {\"positive\": 0, \"negative\": 19, \"ratio\": 0.05},\n",
    "                \"♛\": {\"positive\": 0, \"negative\": 210, \"ratio\": 0.004739336492890996},\n",
    "                \"》\": {\"positive\": 0, \"negative\": 210, \"ratio\": 0.004739336492890996},\n",
    "                \"beli̇ev\": {\n",
    "                    \"positive\": 0,\n",
    "                    \"negative\": 35,\n",
    "                    \"ratio\": 0.027777777777777776,\n",
    "                },\n",
    "                \"wi̇ll\": {\"positive\": 0, \"negative\": 35, \"ratio\": 0.027777777777777776},\n",
    "                \"justi̇n\": {\n",
    "                    \"positive\": 0,\n",
    "                    \"negative\": 35,\n",
    "                    \"ratio\": 0.027777777777777776,\n",
    "                },\n",
    "                \"ｓｅｅ\": {\"positive\": 0, \"negative\": 35, \"ratio\": 0.027777777777777776},\n",
    "                \"ｍｅ\": {\"positive\": 0, \"negative\": 35, \"ratio\": 0.027777777777777776},\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"default_check2\",\n",
    "            \"input\": {\"freqs\": freqs, \"label\": 1, \"threshold\": 10},\n",
    "            \"expected\": {\n",
    "                \"followfriday\": {\"positive\": 23, \"negative\": 0, \"ratio\": 24.0},\n",
    "                \"commun\": {\"positive\": 27, \"negative\": 1, \"ratio\": 14.0},\n",
    "                \":)\": {\"positive\": 2960, \"negative\": 2, \"ratio\": 987.0},\n",
    "                \"flipkartfashionfriday\": {\"positive\": 16, \"negative\": 0, \"ratio\": 17.0},\n",
    "                \":D\": {\"positive\": 523, \"negative\": 0, \"ratio\": 524.0},\n",
    "                \":p\": {\"positive\": 104, \"negative\": 0, \"ratio\": 105.0},\n",
    "                \"influenc\": {\"positive\": 16, \"negative\": 0, \"ratio\": 17.0},\n",
    "                \":-)\": {\"positive\": 552, \"negative\": 0, \"ratio\": 553.0},\n",
    "                \"here'\": {\"positive\": 20, \"negative\": 0, \"ratio\": 21.0},\n",
    "                \"youth\": {\"positive\": 14, \"negative\": 0, \"ratio\": 15.0},\n",
    "                \"bam\": {\"positive\": 44, \"negative\": 0, \"ratio\": 45.0},\n",
    "                \"warsaw\": {\"positive\": 44, \"negative\": 0, \"ratio\": 45.0},\n",
    "                \"shout\": {\"positive\": 11, \"negative\": 0, \"ratio\": 12.0},\n",
    "                \";)\": {\"positive\": 22, \"negative\": 0, \"ratio\": 23.0},\n",
    "                \"stat\": {\"positive\": 51, \"negative\": 0, \"ratio\": 52.0},\n",
    "                \"arriv\": {\"positive\": 57, \"negative\": 4, \"ratio\": 11.6},\n",
    "                \"glad\": {\"positive\": 41, \"negative\": 2, \"ratio\": 14.0},\n",
    "                \"blog\": {\"positive\": 27, \"negative\": 0, \"ratio\": 28.0},\n",
    "                \"fav\": {\"positive\": 11, \"negative\": 0, \"ratio\": 12.0},\n",
    "                \"fantast\": {\"positive\": 9, \"negative\": 0, \"ratio\": 10.0},\n",
    "                \"fback\": {\"positive\": 26, \"negative\": 0, \"ratio\": 27.0},\n",
    "                \"pleasur\": {\"positive\": 10, \"negative\": 0, \"ratio\": 11.0},\n",
    "                \"←\": {\"positive\": 9, \"negative\": 0, \"ratio\": 10.0},\n",
    "                \"aqui\": {\"positive\": 9, \"negative\": 0, \"ratio\": 10.0},\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"low_threshold_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"label\": 0, \"threshold\": 0.01},\n",
    "            \"expected\": {\n",
    "                \":(\": {\"positive\": 1, \"negative\": 3675, \"ratio\": 0.000544069640914037},\n",
    "                \":-(\": {\"positive\": 0, \"negative\": 386, \"ratio\": 0.002583979328165375},\n",
    "                \"♛\": {\"positive\": 0, \"negative\": 210, \"ratio\": 0.004739336492890996},\n",
    "                \"》\": {\"positive\": 0, \"negative\": 210, \"ratio\": 0.004739336492890996},\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"high_threshold_check\",\n",
    "            \"input\": {\"freqs\": freqs, \"label\": 1, \"threshold\": 30},\n",
    "            \"expected\": {\n",
    "                \":)\": {\"positive\": 2960, \"negative\": 2, \"ratio\": 987.0},\n",
    "                \":D\": {\"positive\": 523, \"negative\": 0, \"ratio\": 524.0},\n",
    "                \":p\": {\"positive\": 104, \"negative\": 0, \"ratio\": 105.0},\n",
    "                \":-)\": {\"positive\": 552, \"negative\": 0, \"ratio\": 553.0},\n",
    "                \"bam\": {\"positive\": 44, \"negative\": 0, \"ratio\": 45.0},\n",
    "                \"warsaw\": {\"positive\": 44, \"negative\": 0, \"ratio\": 45.0},\n",
    "                \"stat\": {\"positive\": 51, \"negative\": 0, \"ratio\": 52.0},\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result, dict)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": type(test_case[\"expected\"]),\n",
    "                    \"got\": type(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong output type.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert len(result) == len(test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": len(test_case[\"expected\"]),\n",
    "                    \"got\": len(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Wrong number of elements in output dictionary.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        for key, value in test_case[\"expected\"].items():\n",
    "            for sec_key, sec_value in value.items():\n",
    "                try:\n",
    "                    assert np.isclose(result[key][sec_key], sec_value)\n",
    "                    successful_cases += 1\n",
    "                except:\n",
    "                    failed_cases.append(\n",
    "                        {\n",
    "                            \"name\": test_case[\"name\"],\n",
    "                            \"expected\": {key: {sec_key: sec_value}},\n",
    "                            \"got\": {key: result[key][sec_key]},\n",
    "                        }\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Wrong output values.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}.\"\n",
    "                    )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
